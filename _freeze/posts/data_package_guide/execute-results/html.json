{
  "hash": "cc2f954611d5bf660c908c739722bce7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Writing R packages with large datasets\"\ndate: \"2024-04-11\"\ncategories: [package, data]\n---\n\n\n\nThis post aims to be a guide for those writing R packages that will contain datasets, presenting some approaches and solutions for some pitfalls.\n\nR packages commonly contain some datasets. They are useful to provide functions examples of usage and also internally test the package functions.\n\n## Creating package data files\n\nFollowing the [R Packages book's recommendation](https://r-pkgs.org/data.html), you may use\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusethis::use_data_raw()\n```\n:::\n\n\n\nThis command will create a `data-raw` folder on your package and an R script. The idea is to put all steps necessary to create the data there. But this folder is not used to build your package. For this reason, at the end of this script, you will find this command\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusethis::use_data()\n```\n:::\n\n\n\nThe `use_data()` command will write the final data to the official data folder of the package.\n\n## Data size restrictions\n\nConsidering CRAN guidelines, a package should be small. Remember that a package, once on CRAN, will be compiled to some operational systems and all package versions are archived. Thus, the package sources, binaries and its versions will occupy more space than you think. For this reason, it is asked to have less than 5MB of data. If you need more, you will need to ask for exception and present a good justification.\n\nYou can try different compression methods with the `use_data()` command to get smaller files. On my experience, `xz` compressions gives you smaller files with data frames.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusethis::use_data(..., compress = \"gzip\") # The default\nusethis::use_data(..., compress = \"bzip2\")\nusethis::use_data(..., compress = \"xz\")\n```\n:::\n\n\n\n## Document your data\n\nRemember to create an R file at the R folder of your package with the documentation of your dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusethis::use_r(\"mydata\")\n```\n:::\n\n\n\nAnd inside it, describe your data. An example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' My data name\n#'\n#' Some description.\n#'\n#' \\describe{\n#'   \\item{variable_1}{description}\n#'   \\item{variable_2}{description}\n#'   \\item{variable_3}{description}\n#' }\n\"mydata\"\n```\n:::\n\n\n\n## Separate packages\n\nTo reduce the load of versioning packages with data, some package authors create two packages: a main package with functions, and a data package with only the data.\n\nThe main package will import the data package. The vantage here is that you can upgrade your package without creating useless versions of your data.\n\n## Solutions for large datasets\n\nUntil now, those are the standard procedures to include small datasets with your package. But what to do when you need large files, occupying more than 5MB?\n\n### The `piggyback` solution\n\nThere is a nice package called [piggyback](https://docs.ropensci.org/piggyback/). It provides functions to store files as GitHub releases. As Git and GitHub are not ideal to version data, it uses the GitHub releases feature to host files.\n\nWith this package, you can store your large datasets as a GitHub release and download them on your package.\n\n### The `pins` solution\n\nThe package [pins](https://pins.rstudio.com) present very interesting ways to publish data, models and R objects on \"boards\". You can store your data on Azure, Google Drive, Google Cloud Storage, Amazon's S3 and others.\n\nSomething very useful on `pins` is its capacity to cache files. This means that once the file was downloaded, the user will not need to download it again, as it will be cached on the computer.\n\nThe package has a nice article about web-hosted boards, including a [suggestion](https://pins.rstudio.com/articles/using-board-url.html#pkgdown) to use the a pkgdown asset to store your data. If you store your `pkgdown` on GitHub, remember the file size limits (usually ok for less than 20MB).\n\n### My solution: `zendown`\n\n[Zenodo](https://zenodo.org) is a scientific data repository maintained by CERN. It is very stable and easy to use. You create an account, make an upload and instantly your dataset is available to other people to download. Also, your data gets a nice DOI code for citation\n\nI created a package called [zendown](https://rfsaldanha.github.io/zendown/), with functions to access data stored at Zenodo. The package will download and cache the requested data, avoiding unnecessary downloads.\n\nIts usage is very straighfoward. You just need the Zenodo deposit code (the number on the end of the URL of your repository) and the desired file name.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://zenodo.org/records/10959197\nmy_iris <- zendown::zen_file(deposit_id = 10959197, file_name = \"iris.rds\") |>\n  readRDS()\n\nhead(my_iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n\n\n## Using `zendown` in a package\n\nAfter importing `zendown` in your package (`usethis::use_package(\"zendown\")`), create an R file just like the documentation example, but it will contain a function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' My data name\n#'\n#' Some description.\n#'\n#' \\describe{\n#'   \\item{variable_1}{description}\n#'   \\item{variable_2}{description}\n#'   \\item{variable_3}{description}\n#' }\nmydata <- function(){\n  path <- zendown::zen_file(10959197,  \"iris.rds\")\n  readRDS(file = path)\n}\n```\n:::\n\n\n\nModify the deposit ID and file name and adapt the read function accordingly to your data. It can be a CSV, parquet file, etc.\n\nThen, on your package functions, call your data function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' My Fuction\n#'\n#' Some description.\n#'\n#' @param x\nmyfunction <- function(x){\n  df <- mydata()\n  df$Sepal.Length * x\n  \n  return(df)\n}\n```\n:::\n\n\n\nAs `zendown` cache the files, the user will download the data just once, and it will be always available to the package.\n\n## The Internet caveat\n\nAll of those solutions depends on an Internet connection, and now your package will rely on the Internet to work. Remember to use functions to verify if Internet is available.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurl::has_internet()\n```\n:::\n\n\n\nAnd to check if the host is online.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRCurl::url.exists(url = \"...\")\n```\n:::\n\n\n\n## Package tests\n\nAs your package relies on Internet now, it can be important to skip some tests on CRAN, as the host may be offline or some connection problem can occur. Include `testthat::skip_on_cran()` on the tests that will require online data.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}