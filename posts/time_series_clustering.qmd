---
title: "Univariate and multivariate time series clustering"
subtitle: "Examples with Brazilian climate data"
date: "2023-11-14"
categories: [clustering, time series, dtwclust]
---

On this post we will try some strategies to cluster univariate and multivariate time series in R with the `{dtwclust}` package.

## Packages

Let's load some useful packages.

```{r}
#| message: false
library(tidyr)    #<1>
library(dplyr)    #<2>
library(ggplot2)  #<3>
library(timetk)   #<4>
library(arrow)    #<5>
library(dtwclust) #<6>
library(geobr)    #<7>
library(sf)       #<8>
```

## Data

### States and capital general data

We will use some spatial data to plot the clustering results. The `uf_sf` contains the geographical boundaries from the Brazilian states.

```{r}
uf_sf <- read_state(showProgress = FALSE)
```

The `cod_cap` object (code folded bellow) is a data frame with Brazilian capitals name, codes and geographical coordinates.

```{r}
#| code-fold: true
cod_cap <- data.frame(
  name_muni = c("Porto Velho, RO", "Manaus, AM", "Rio Branco, AC", 
                "Campo Grande, MS", "Macapá, AP", "Brasília, DF",
                "Boa Vista, RR", "Cuiabá, MT", "Palmas, TO",
                "São Paulo, SP", "Teresina, PI", "Rio de Janeiro, RJ",
                "Belém, PA", "Goiânia, GO", "Salvador, BA", 
                "Florianópolis, SC", "São Luiz, MA", "Maceió, AL",
                "Porto Alegre, RS", "Curitiba, PR", "Belo Horizonte, MG",
                "Fortaleza, CE", "Recife, PE", "João Pessoa, PB",
                "Aracaju, SE", "Natal, RN", "Vitória, ES"),
  code_muni = c(1100205, 1302603, 1200401, 5002704, 1600303,
                5300108, 1400100, 5103403, 1721000, 3550308,
                2211001, 3304557, 1501402, 5208707, 2927408, 
                4205407, 2111300, 2704302, 4314902, 4106902, 
                3106200, 2304400, 2611606, 2507507, 2800308,
                2408102, 3205309),
  lat = c(-8.760770, -3.118660, -9.974990, 
          -20.448600, 0.034934, -15.779500, 
          2.823840, -15.601000, -10.240000,
          -23.532900, -5.091940, -22.912900, 
          -1.455400, -16.686400, -12.971800, 
          -27.594500, -2.538740, -9.665990, 
          -30.031800, -25.419500, -19.910200, 
          -3.716640, -8.046660, -7.115090,
          -10.909100, -5.793570, -20.315500),
  lon = c(-63.8999, -60.0212, -67.8243, 
          -54.6295, -51.0694, -47.9297, 
          -60.6753, -56.0974, -48.3558,
          -46.6395, -42.8034, -43.2003, 
          -48.4898, -49.2643, -38.5011, 
          -48.5477, -44.2825, -35.7350,
          -51.2065, -49.2646, -43.9266, 
          -38.5423, -34.8771, -34.8641,
          -37.0677, -35.1986, -40.3128)
) %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4236)
```

### Climate data

We use climate data from the Brazilian capitals as example dataset. The parquet files are from the `brclimr` package and can be downloaded [here](https://rfsaldanha.github.io/brclimr/articles/parquet_files.html).

```{r}
clim <- open_dataset( #<1>
  sources = c("../../brclim/parquet/brdwgd/pr.parquet", #<1>
              "../../brclim/parquet/brdwgd/tmax.parquet", #<1>
              "../../brclim/parquet/brdwgd/tmin.parquet") #<1>
) %>% #<1>
  filter(date >= as.Date("2011-01-01") & #<2>
           date < as.Date("2021-01-01")) %>% #<2>
  filter(name %in% c("pr_sum", "Tmax_mean", "Tmin_mean")) %>%  #<3>
  filter(code_muni %in% cod_cap$code_muni) %>%  #<4>
  collect() %>%  #<5>
  group_by(code_muni) %>% #<6>
  arrange(date) %>% #<7>
  pivot_wider(names_from = name, values_from = value) %>%  #<8>
  summarise_by_time( #<9>
    .date_var = date, #<9>
    .by = "week", #<9>
    pr_sum = sum(pr_sum, na.rm = TRUE), #<9>
    Tmax_mean = mean(Tmax_mean, na.rm = TRUE), #<9>
    Tmin_mean = mean(Tmin_mean, na.rm = TRUE), #<9>
  ) %>% #<9>
  pivot_longer(cols = c(pr_sum, Tmax_mean, Tmin_mean)) %>% #<10>
  ungroup() %>% #<11>
  arrange(code_muni, name, date) #<12>
```

1.  Open parquet files without loading them to the memory.
2.  The parquet files contains daily data from 1961 to 2020. Let's filter the most 10 years recent data.
3.  Those parquet files contains several zonal statistics, let's keep only the precipitation sum and maximun and minimum temperature averages.
4.  Keep only data from the Brazilian capitals.
5.  Based on the filters above, load the remaining data into memory.
6.  Now we need to do some data manipulation per municipality.
7.  Order the municipality data per date.
8.  Pivot the data to wide format. The precipitation, maximum and minimum temperature data will form three columns, one for each.
9.  We will use the `summarise_by_time` from the {`timetk}` package to aggregate the daily data into weekly aggregates. This will help the clustering algorithm with a less noisy data.
10. Now we pivot the data back to the original long format.
11. Ungroup the data.
12. Arrange the data by municipality code, indicator's name, and date.

Let's take a look at the resulting dataset.

```{r}
glimpse(clim)
```

## Univariate clustering

First, we will cluster the Brazilian capitals based only on the maximum temperature data. We need to extract this specific data from the above dataset and prepare it to be used by the `dtwclust` package, which requires a matrix of time series.

```{r}
uclim <- clim %>% #<1>
  filter(name == "Tmax_mean") %>%  #<2>
  arrange(code_muni, date) %>%  #<3>
  select(-name) %>%  #<4>
  pivot_wider(names_from = "code_muni", values_from = "value") %>%  #<5>
  select(-date) %>%  #<6>
  t() %>% #<7>
  tslist() #<8>
```

1.  Load the `clim` dataset that was just created.
2.  Filter the average maximum temperature data.
3.  Arrange the data by municipality code and date. This will be important as we will see later bellow.
4.  As we have only one indicator, we can remove the indicator name variable from the dataset.
5.  Pivot the dataset into wide format.
6.  Now we remove the date variable, as the `{dtwclust}` package functions will not use it. For this package, the time series is implicit by the sequence of the values.
7.  Now we transpose the data. This will result in a matrix.
8.  And now we coarce this matrix in a list of time series recognized by the `{dtwclust}` package.

Let's see the result.

```{r}
glimpse(uclim)
```

The `uclim` object is list of vectors. Each element of the list is named with the municipality code and contains the time series of the average maximum temperature.

Now we will cluster the capitals based on the maximum temperature averagres using the `tsclust` function from the `{dtwclust}` package. This functions accepts several arguments variations and the [package vignette](https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf) reading is recommended.

We will cluster the municipalities from 2 to 10 groups partitions using the Soft-DTW algorithm.

```{r}
uclust <- tsclust(
  series = uclim, 
  type = "partitional",
  k = 2:10, 
  distance = "sdtw", 
  seed = 13
)
```

Now let's check the validity indices of each \$k\$ cluster approach. The table bellow is sorted by the silhouette statistic.

```{r}
names(uclust) <- paste0("k_", 2:10)
res_cvi <- sapply(uclust, cvi, type = "internal") %>% 
  t() %>%
  as_tibble(rownames = "k") %>%
  arrange(-Sil)

res_cvi
```

Considering the silhouette statistic, the $k=3$ clustering results present the best results. Let's plot the results from it.

```{r}
u_sel_clust <- uclust[[res_cvi[[1,1]]]]

plot(u_sel_clust)
```

At this plot, each line is the time series of one capital. We see that partition #3 contains the smaller maximum averages values and partition \$2 contains the higher maximum averages values. The table bellow tell us how many capitals are inside into each partition.

```{r}
table(u_sel_clust@cluster)
```

Let's see those results in a map. First we extract from the clustering result the labeled partitions and join it with the capitals metadata, and then plot the cluster partitions in a map

```{r}
u_cluster_ids <- tibble(
  code_muni = as.numeric(names(uclim)),
  group = as.character(u_sel_clust@cluster)
) %>% 
  left_join(cod_cap, by = "code_muni") %>%
  arrange(group, name_muni) %>%
  st_as_sf()
```

```{r}
ggplot() +
  geom_sf(data = uf_sf, fill = "lightgray", color = "grey20", size=.15, show.legend = FALSE) +
  geom_sf(data = u_cluster_ids, aes(color = group), size = 3) +
  theme_minimal()
```

Partition #1 contains some capitals from the north and central regions. The partition #2 is formed from some capitals from the northeast, southeast and central regions. Partition #3 contains the some capitals from the southeast and south regions.

Interesting to note that capitals from central regions are clustered together with capitals from the northeast considering only the average maximum temperature.

## Multivariate clustering

Now we will cluster the Brazilian capitals based on more climate indicators, by using the precipitation, maximum and minimum temperature.

```{r}
mclim_g <- clim %>% #<1>
  arrange(name, code_muni, date) %>% #<2>
  group_by(code_muni) %>% #<3>
  pivot_wider(names_from = "name", values_from = "value") %>% #<4>
  mutate(across(c(pr_sum, Tmax_mean, Tmin_mean), ~ standardize_vec(.x, silent = TRUE))) %>% #<5>
  select(-date) #<6>

mclim <- group_split(mclim_g, .keep = FALSE) %>% #<7>
  tslist(simplify = TRUE) #<8>

names(mclim) <- group_keys(mclim_g)$code_muni #<9>
```

1.  Load the `clim` object with all the data.
2.  Arrange the values by indicator's name, municipality code and date.
3.  Group the dataset to perform some manipulation on municipality level.
4.  Pivot the indicators to a wide format.
5.  The clustering algorithms may be influenced by the magnitude and units of the indicators. We will standarize the values with the `standardize_vec` from the `{timetk}` package.
6.  Remove the date variable.
7.  The `mclim_g` object is a grouped data frame. We will split it to list with the `group_split` function, removing the `code_muni` variable (`keep = FALSE` argument).
8.  Then we coerce this to a time series list.
9.  Finally, we update the list names with the municipality codes, which are the grouping keys from the `mclim_g` intermediary object.

Let's take a look at the first three list elements of the `mclim` object.

```{r}
glimpse(mclim[1:3])
```

Each object of the named list is a named matrix with the temperature and precipitation data.

We will use the same clustering function, but passing the multivariate list as series argument. Next, we will check the silhouette statistic from the different partition numbers.

```{r}
mclust <- tsclust(
  series = mclim, 
  type = "partitional", k = 2:10, 
  distance = "sdtw", 
  seed = 13
)
```

```{r}
names(mclust) <- paste0("k_", 2:10)
res_cvi <- sapply(mclust, cvi, type = "internal") %>% 
  t() %>%
  as_tibble(rownames = "k") %>%
  arrange(-Sil)

res_cvi
```

$k=3$ partitions present the higher silhouette statistic. Let's plot it.

```{r}
m_sel_clust <- mclust[[res_cvi[[1,1]]]]

plot(m_sel_clust)
```

The plot present the time series from each capital per partition. On each subplot there are three sections, one for each indicator (minimum temperature, maximum temperature and precipitation).

The table bellow shows how many capitals are inside each partition.

```{r}
table(m_sel_clust@cluster)
```

Now, let's plot these results in a map.

```{r}
m_cluster_ids <- tibble(
  code_muni = as.numeric(names(mclim)),
  group = as.character(m_sel_clust@cluster)
) %>% 
  left_join(cod_cap, by = "code_muni") %>%
  arrange(group, name_muni) %>%
  st_as_sf()

m_cluster_ids
```

```{r}
ggplot() +
  geom_sf(data = uf_sf, fill = "lightgray", color = "grey20", size=.15, show.legend = FALSE) +
  geom_sf(data = m_cluster_ids, aes(color = group), size = 3) +
  theme_minimal()
```

The multivariate clustering result appears to be more interesting. The partition #1 is formed by the capital from the northeast, partition #2 with capitals from more central regions (except from Teresina in the north), partition #3 with capitals from the north, and partition #4 with capitals from the southeast and south capitals.

## Session info

```{r}
sessionInfo()
```
